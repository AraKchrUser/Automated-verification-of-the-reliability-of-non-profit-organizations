{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c17da205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ALBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ALL_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'Adafactor',\n",
       " 'AdamW',\n",
       " 'AdaptiveEmbedding',\n",
       " 'AlbertConfig',\n",
       " 'AlbertForMaskedLM',\n",
       " 'AlbertForMultipleChoice',\n",
       " 'AlbertForPreTraining',\n",
       " 'AlbertForQuestionAnswering',\n",
       " 'AlbertForSequenceClassification',\n",
       " 'AlbertForTokenClassification',\n",
       " 'AlbertModel',\n",
       " 'AlbertPreTrainedModel',\n",
       " 'AlbertTokenizer',\n",
       " 'AutoConfig',\n",
       " 'AutoModel',\n",
       " 'AutoModelForCausalLM',\n",
       " 'AutoModelForMaskedLM',\n",
       " 'AutoModelForMultipleChoice',\n",
       " 'AutoModelForPreTraining',\n",
       " 'AutoModelForQuestionAnswering',\n",
       " 'AutoModelForSeq2SeqLM',\n",
       " 'AutoModelForSequenceClassification',\n",
       " 'AutoModelForTokenClassification',\n",
       " 'AutoModelWithLMHead',\n",
       " 'AutoTokenizer',\n",
       " 'BART_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'BERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'BartConfig',\n",
       " 'BartForConditionalGeneration',\n",
       " 'BartForQuestionAnswering',\n",
       " 'BartForSequenceClassification',\n",
       " 'BartModel',\n",
       " 'BartTokenizer',\n",
       " 'BartTokenizerFast',\n",
       " 'BasicTokenizer',\n",
       " 'BatchEncoding',\n",
       " 'BertConfig',\n",
       " 'BertForMaskedLM',\n",
       " 'BertForMultipleChoice',\n",
       " 'BertForNextSentencePrediction',\n",
       " 'BertForPreTraining',\n",
       " 'BertForQuestionAnswering',\n",
       " 'BertForSequenceClassification',\n",
       " 'BertForTokenClassification',\n",
       " 'BertJapaneseTokenizer',\n",
       " 'BertLMHeadModel',\n",
       " 'BertLayer',\n",
       " 'BertModel',\n",
       " 'BertPreTrainedModel',\n",
       " 'BertTokenizer',\n",
       " 'BertTokenizerFast',\n",
       " 'CAMEMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CAMEMBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CONFIG_MAPPING',\n",
       " 'CONFIG_NAME',\n",
       " 'CTRLConfig',\n",
       " 'CTRLLMHeadModel',\n",
       " 'CTRLModel',\n",
       " 'CTRLPreTrainedModel',\n",
       " 'CTRLTokenizer',\n",
       " 'CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'CTRL_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'CamembertConfig',\n",
       " 'CamembertForCausalLM',\n",
       " 'CamembertForMaskedLM',\n",
       " 'CamembertForMultipleChoice',\n",
       " 'CamembertForQuestionAnswering',\n",
       " 'CamembertForSequenceClassification',\n",
       " 'CamembertForTokenClassification',\n",
       " 'CamembertModel',\n",
       " 'CamembertTokenizer',\n",
       " 'CharSpan',\n",
       " 'CharacterTokenizer',\n",
       " 'Conv1D',\n",
       " 'Conversation',\n",
       " 'ConversationalPipeline',\n",
       " 'CsvPipelineDataFormat',\n",
       " 'DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DISTILBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'DPRConfig',\n",
       " 'DPRContextEncoder',\n",
       " 'DPRContextEncoderTokenizer',\n",
       " 'DPRContextEncoderTokenizerFast',\n",
       " 'DPRPretrainedContextEncoder',\n",
       " 'DPRPretrainedQuestionEncoder',\n",
       " 'DPRPretrainedReader',\n",
       " 'DPRQuestionEncoder',\n",
       " 'DPRQuestionEncoderTokenizer',\n",
       " 'DPRQuestionEncoderTokenizerFast',\n",
       " 'DPRReader',\n",
       " 'DPRReaderTokenizer',\n",
       " 'DPRReaderTokenizerFast',\n",
       " 'DPR_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'DataCollator',\n",
       " 'DataCollatorForLanguageModeling',\n",
       " 'DataCollatorForNextSentencePrediction',\n",
       " 'DataCollatorForPermutationLanguageModeling',\n",
       " 'DataCollatorWithPadding',\n",
       " 'DataProcessor',\n",
       " 'DistilBertConfig',\n",
       " 'DistilBertForMaskedLM',\n",
       " 'DistilBertForMultipleChoice',\n",
       " 'DistilBertForQuestionAnswering',\n",
       " 'DistilBertForSequenceClassification',\n",
       " 'DistilBertForTokenClassification',\n",
       " 'DistilBertModel',\n",
       " 'DistilBertPreTrainedModel',\n",
       " 'DistilBertTokenizer',\n",
       " 'DistilBertTokenizerFast',\n",
       " 'ELECTRA_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ELECTRA_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ElectraConfig',\n",
       " 'ElectraForMaskedLM',\n",
       " 'ElectraForMultipleChoice',\n",
       " 'ElectraForPreTraining',\n",
       " 'ElectraForQuestionAnswering',\n",
       " 'ElectraForSequenceClassification',\n",
       " 'ElectraForTokenClassification',\n",
       " 'ElectraModel',\n",
       " 'ElectraPreTrainedModel',\n",
       " 'ElectraTokenizer',\n",
       " 'ElectraTokenizerFast',\n",
       " 'EncoderDecoderConfig',\n",
       " 'EncoderDecoderModel',\n",
       " 'EvalPrediction',\n",
       " 'FLAUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'FLAUBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'FeatureExtractionPipeline',\n",
       " 'FillMaskPipeline',\n",
       " 'FlaubertConfig',\n",
       " 'FlaubertForMultipleChoice',\n",
       " 'FlaubertForQuestionAnswering',\n",
       " 'FlaubertForQuestionAnsweringSimple',\n",
       " 'FlaubertForSequenceClassification',\n",
       " 'FlaubertForTokenClassification',\n",
       " 'FlaubertModel',\n",
       " 'FlaubertTokenizer',\n",
       " 'FlaubertWithLMHeadModel',\n",
       " 'GPT2Config',\n",
       " 'GPT2DoubleHeadsModel',\n",
       " 'GPT2LMHeadModel',\n",
       " 'GPT2Model',\n",
       " 'GPT2PreTrainedModel',\n",
       " 'GPT2Tokenizer',\n",
       " 'GPT2TokenizerFast',\n",
       " 'GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'GPT2_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'GlueDataTrainingArguments',\n",
       " 'GlueDataset',\n",
       " 'HfArgumentParser',\n",
       " 'InputExample',\n",
       " 'InputFeatures',\n",
       " 'JsonPipelineDataFormat',\n",
       " 'LONGFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'LONGFORMER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'LineByLineTextDataset',\n",
       " 'LongformerConfig',\n",
       " 'LongformerForMaskedLM',\n",
       " 'LongformerForMultipleChoice',\n",
       " 'LongformerForQuestionAnswering',\n",
       " 'LongformerForSequenceClassification',\n",
       " 'LongformerForTokenClassification',\n",
       " 'LongformerModel',\n",
       " 'LongformerSelfAttention',\n",
       " 'LongformerTokenizer',\n",
       " 'LongformerTokenizerFast',\n",
       " 'MBartConfig',\n",
       " 'MBartForConditionalGeneration',\n",
       " 'MBartTokenizer',\n",
       " 'MMBTConfig',\n",
       " 'MMBTForClassification',\n",
       " 'MMBTModel',\n",
       " 'MOBILEBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'MOBILEBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'MODEL_CARD_NAME',\n",
       " 'MODEL_FOR_CAUSAL_LM_MAPPING',\n",
       " 'MODEL_FOR_MASKED_LM_MAPPING',\n",
       " 'MODEL_FOR_MULTIPLE_CHOICE_MAPPING',\n",
       " 'MODEL_FOR_PRETRAINING_MAPPING',\n",
       " 'MODEL_FOR_QUESTION_ANSWERING_MAPPING',\n",
       " 'MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING',\n",
       " 'MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING',\n",
       " 'MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING',\n",
       " 'MODEL_MAPPING',\n",
       " 'MODEL_WITH_LM_HEAD_MAPPING',\n",
       " 'MarianConfig',\n",
       " 'MarianMTModel',\n",
       " 'MarianTokenizer',\n",
       " 'MecabTokenizer',\n",
       " 'MobileBertConfig',\n",
       " 'MobileBertForMaskedLM',\n",
       " 'MobileBertForMultipleChoice',\n",
       " 'MobileBertForNextSentencePrediction',\n",
       " 'MobileBertForPreTraining',\n",
       " 'MobileBertForQuestionAnswering',\n",
       " 'MobileBertForSequenceClassification',\n",
       " 'MobileBertForTokenClassification',\n",
       " 'MobileBertLayer',\n",
       " 'MobileBertModel',\n",
       " 'MobileBertPreTrainedModel',\n",
       " 'MobileBertTokenizer',\n",
       " 'MobileBertTokenizerFast',\n",
       " 'ModalEmbeddings',\n",
       " 'ModelCard',\n",
       " 'NerPipeline',\n",
       " 'OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'OpenAIGPTConfig',\n",
       " 'OpenAIGPTDoubleHeadsModel',\n",
       " 'OpenAIGPTLMHeadModel',\n",
       " 'OpenAIGPTModel',\n",
       " 'OpenAIGPTPreTrainedModel',\n",
       " 'OpenAIGPTTokenizer',\n",
       " 'OpenAIGPTTokenizerFast',\n",
       " 'PYTORCH_PRETRAINED_BERT_CACHE',\n",
       " 'PYTORCH_TRANSFORMERS_CACHE',\n",
       " 'PegasusConfig',\n",
       " 'PegasusForConditionalGeneration',\n",
       " 'PegasusTokenizer',\n",
       " 'PipedPipelineDataFormat',\n",
       " 'Pipeline',\n",
       " 'PipelineDataFormat',\n",
       " 'PreTrainedModel',\n",
       " 'PreTrainedTokenizer',\n",
       " 'PreTrainedTokenizerBase',\n",
       " 'PreTrainedTokenizerFast',\n",
       " 'PretrainedBartModel',\n",
       " 'PretrainedConfig',\n",
       " 'PyTorchBenchmark',\n",
       " 'PyTorchBenchmarkArguments',\n",
       " 'QuestionAnsweringPipeline',\n",
       " 'REFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'REFORMER_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'RETRIBERT_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'RETRIBERT_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'ReformerAttention',\n",
       " 'ReformerConfig',\n",
       " 'ReformerForMaskedLM',\n",
       " 'ReformerForQuestionAnswering',\n",
       " 'ReformerForSequenceClassification',\n",
       " 'ReformerLayer',\n",
       " 'ReformerModel',\n",
       " 'ReformerModelWithLMHead',\n",
       " 'ReformerTokenizer',\n",
       " 'RetriBertConfig',\n",
       " 'RetriBertModel',\n",
       " 'RetriBertPreTrainedModel',\n",
       " 'RetriBertTokenizer',\n",
       " 'RetriBertTokenizerFast',\n",
       " 'RobertaConfig',\n",
       " 'RobertaForCausalLM',\n",
       " 'RobertaForMaskedLM',\n",
       " 'RobertaForMultipleChoice',\n",
       " 'RobertaForQuestionAnswering',\n",
       " 'RobertaForSequenceClassification',\n",
       " 'RobertaForTokenClassification',\n",
       " 'RobertaModel',\n",
       " 'RobertaTokenizer',\n",
       " 'RobertaTokenizerFast',\n",
       " 'SPIECE_UNDERLINE',\n",
       " 'SingleSentenceClassificationProcessor',\n",
       " 'SpecialTokensMixin',\n",
       " 'SquadDataTrainingArguments',\n",
       " 'SquadDataset',\n",
       " 'SquadExample',\n",
       " 'SquadFeatures',\n",
       " 'SquadV1Processor',\n",
       " 'SquadV2Processor',\n",
       " 'SummarizationPipeline',\n",
       " 'T5Config',\n",
       " 'T5ForConditionalGeneration',\n",
       " 'T5Model',\n",
       " 'T5PreTrainedModel',\n",
       " 'T5Tokenizer',\n",
       " 'T5_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'T5_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'TF2_WEIGHTS_NAME',\n",
       " 'TFTrainingArguments',\n",
       " 'TF_WEIGHTS_NAME',\n",
       " 'TOKENIZER_MAPPING',\n",
       " 'TRANSFORMERS_CACHE',\n",
       " 'TRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'TensorType',\n",
       " 'TextClassificationPipeline',\n",
       " 'TextDataset',\n",
       " 'TextDatasetForNextSentencePrediction',\n",
       " 'TextGenerationPipeline',\n",
       " 'TokenClassificationPipeline',\n",
       " 'TokenSpan',\n",
       " 'Trainer',\n",
       " 'TrainingArguments',\n",
       " 'TransfoXLConfig',\n",
       " 'TransfoXLCorpus',\n",
       " 'TransfoXLLMHeadModel',\n",
       " 'TransfoXLModel',\n",
       " 'TransfoXLPreTrainedModel',\n",
       " 'TransfoXLTokenizer',\n",
       " 'TransfoXLTokenizerFast',\n",
       " 'TranslationPipeline',\n",
       " 'WEIGHTS_NAME',\n",
       " 'WordpieceTokenizer',\n",
       " 'XLMConfig',\n",
       " 'XLMForMultipleChoice',\n",
       " 'XLMForQuestionAnswering',\n",
       " 'XLMForQuestionAnsweringSimple',\n",
       " 'XLMForSequenceClassification',\n",
       " 'XLMForTokenClassification',\n",
       " 'XLMModel',\n",
       " 'XLMPreTrainedModel',\n",
       " 'XLMRobertaConfig',\n",
       " 'XLMRobertaForMaskedLM',\n",
       " 'XLMRobertaForMultipleChoice',\n",
       " 'XLMRobertaForQuestionAnswering',\n",
       " 'XLMRobertaForSequenceClassification',\n",
       " 'XLMRobertaForTokenClassification',\n",
       " 'XLMRobertaModel',\n",
       " 'XLMRobertaTokenizer',\n",
       " 'XLMTokenizer',\n",
       " 'XLMWithLMHeadModel',\n",
       " 'XLM_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'XLM_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'XLM_ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP',\n",
       " 'XLNET_PRETRAINED_MODEL_ARCHIVE_LIST',\n",
       " 'XLNetConfig',\n",
       " 'XLNetForMultipleChoice',\n",
       " 'XLNetForQuestionAnswering',\n",
       " 'XLNetForQuestionAnsweringSimple',\n",
       " 'XLNetForSequenceClassification',\n",
       " 'XLNetForTokenClassification',\n",
       " 'XLNetLMHeadModel',\n",
       " 'XLNetModel',\n",
       " 'XLNetPreTrainedModel',\n",
       " 'XLNetTokenizer',\n",
       " 'ZeroShotClassificationPipeline',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'absl',\n",
       " 'activations',\n",
       " 'add_end_docstrings',\n",
       " 'add_start_docstrings',\n",
       " 'apply_chunking_to_forward',\n",
       " 'benchmark',\n",
       " 'cached_path',\n",
       " 'configuration_albert',\n",
       " 'configuration_auto',\n",
       " 'configuration_bart',\n",
       " 'configuration_bert',\n",
       " 'configuration_camembert',\n",
       " 'configuration_ctrl',\n",
       " 'configuration_distilbert',\n",
       " 'configuration_dpr',\n",
       " 'configuration_electra',\n",
       " 'configuration_encoder_decoder',\n",
       " 'configuration_flaubert',\n",
       " 'configuration_gpt2',\n",
       " 'configuration_longformer',\n",
       " 'configuration_marian',\n",
       " 'configuration_mbart',\n",
       " 'configuration_mmbt',\n",
       " 'configuration_mobilebert',\n",
       " 'configuration_openai',\n",
       " 'configuration_pegasus',\n",
       " 'configuration_reformer',\n",
       " 'configuration_retribert',\n",
       " 'configuration_roberta',\n",
       " 'configuration_t5',\n",
       " 'configuration_transfo_xl',\n",
       " 'configuration_utils',\n",
       " 'configuration_xlm',\n",
       " 'configuration_xlm_roberta',\n",
       " 'configuration_xlnet',\n",
       " 'convert_tf_weight_name_to_pt_weight_name',\n",
       " 'data',\n",
       " 'default_data_collator',\n",
       " 'file_utils',\n",
       " 'generation_utils',\n",
       " 'get_constant_schedule',\n",
       " 'get_constant_schedule_with_warmup',\n",
       " 'get_cosine_schedule_with_warmup',\n",
       " 'get_cosine_with_hard_restarts_schedule_with_warmup',\n",
       " 'get_linear_schedule_with_warmup',\n",
       " 'get_polynomial_decay_schedule_with_warmup',\n",
       " 'glue_compute_metrics',\n",
       " 'glue_convert_examples_to_features',\n",
       " 'glue_output_modes',\n",
       " 'glue_processors',\n",
       " 'glue_tasks_num_labels',\n",
       " 'hf_argparser',\n",
       " 'integrations',\n",
       " 'is_apex_available',\n",
       " 'is_comet_available',\n",
       " 'is_nlp_available',\n",
       " 'is_optuna_available',\n",
       " 'is_psutil_available',\n",
       " 'is_py3nvml_available',\n",
       " 'is_ray_available',\n",
       " 'is_sklearn_available',\n",
       " 'is_tensorboard_available',\n",
       " 'is_tf_available',\n",
       " 'is_torch_available',\n",
       " 'is_torch_tpu_available',\n",
       " 'is_wandb_available',\n",
       " 'load_pytorch_checkpoint_in_tf2_model',\n",
       " 'load_pytorch_model_in_tf2_model',\n",
       " 'load_pytorch_weights_in_tf2_model',\n",
       " 'load_tf2_checkpoint_in_pytorch_model',\n",
       " 'load_tf2_model_in_pytorch_model',\n",
       " 'load_tf2_weights_in_pytorch_model',\n",
       " 'load_tf_weights_in_albert',\n",
       " 'load_tf_weights_in_bert',\n",
       " 'load_tf_weights_in_electra',\n",
       " 'load_tf_weights_in_gpt2',\n",
       " 'load_tf_weights_in_mobilebert',\n",
       " 'load_tf_weights_in_openai_gpt',\n",
       " 'load_tf_weights_in_t5',\n",
       " 'load_tf_weights_in_transfo_xl',\n",
       " 'load_tf_weights_in_xlnet',\n",
       " 'logger',\n",
       " 'logging',\n",
       " 'modelcard',\n",
       " 'modeling_albert',\n",
       " 'modeling_auto',\n",
       " 'modeling_bart',\n",
       " 'modeling_bert',\n",
       " 'modeling_camembert',\n",
       " 'modeling_ctrl',\n",
       " 'modeling_distilbert',\n",
       " 'modeling_dpr',\n",
       " 'modeling_electra',\n",
       " 'modeling_encoder_decoder',\n",
       " 'modeling_flaubert',\n",
       " 'modeling_gpt2',\n",
       " 'modeling_longformer',\n",
       " 'modeling_marian',\n",
       " 'modeling_mbart',\n",
       " 'modeling_mmbt',\n",
       " 'modeling_mobilebert',\n",
       " 'modeling_openai',\n",
       " 'modeling_outputs',\n",
       " 'modeling_pegasus',\n",
       " 'modeling_reformer',\n",
       " 'modeling_retribert',\n",
       " 'modeling_roberta',\n",
       " 'modeling_t5',\n",
       " 'modeling_tf_pytorch_utils',\n",
       " 'modeling_transfo_xl',\n",
       " 'modeling_transfo_xl_utilities',\n",
       " 'modeling_utils',\n",
       " 'modeling_xlm',\n",
       " 'modeling_xlm_roberta',\n",
       " 'modeling_xlnet',\n",
       " 'optimization',\n",
       " 'pipeline',\n",
       " 'pipelines',\n",
       " 'prune_layer',\n",
       " 'set_seed',\n",
       " 'squad_convert_examples_to_features',\n",
       " 'tokenization_albert',\n",
       " 'tokenization_auto',\n",
       " 'tokenization_bart',\n",
       " 'tokenization_bert',\n",
       " 'tokenization_bert_japanese',\n",
       " 'tokenization_camembert',\n",
       " 'tokenization_ctrl',\n",
       " 'tokenization_distilbert',\n",
       " 'tokenization_dpr',\n",
       " 'tokenization_electra',\n",
       " 'tokenization_flaubert',\n",
       " 'tokenization_gpt2',\n",
       " 'tokenization_longformer',\n",
       " 'tokenization_marian',\n",
       " 'tokenization_mbart',\n",
       " 'tokenization_mobilebert',\n",
       " 'tokenization_openai',\n",
       " 'tokenization_pegasus',\n",
       " 'tokenization_reformer',\n",
       " 'tokenization_retribert',\n",
       " 'tokenization_roberta',\n",
       " 'tokenization_t5',\n",
       " 'tokenization_transfo_xl',\n",
       " 'tokenization_utils',\n",
       " 'tokenization_utils_base',\n",
       " 'tokenization_utils_fast',\n",
       " 'tokenization_xlm',\n",
       " 'tokenization_xlm_roberta',\n",
       " 'tokenization_xlnet',\n",
       " 'top_k_top_p_filtering',\n",
       " 'torch_distributed_zero_first',\n",
       " 'trainer',\n",
       " 'trainer_utils',\n",
       " 'training_args',\n",
       " 'training_args_tf',\n",
       " 'utils',\n",
       " 'xnli_compute_metrics',\n",
       " 'xnli_output_modes',\n",
       " 'xnli_processors',\n",
       " 'xnli_tasks_num_labels']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "dir(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28315afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0161d2e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Model name 'IlyaGusev/mbart_ru_sum_gazeta' was not found in tokenizers model name list (facebook/mbart-large-en-ro, facebook/mbart-large-cc25). We assumed 'IlyaGusev/mbart_ru_sum_gazeta' was a path, a model identifier, or url to a directory containing vocabulary files named ['sentencepiece.bpe.model'] but couldn't find such vocabulary files at this path or url.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7698adb43172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marticle_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"IlyaGusev/mbart_ru_sum_gazeta\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMBartTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMBartForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/py3.6/py3.6/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \"\"\"\n\u001b[0;32m-> 1425\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/py3.6/py3.6/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1529\u001b[0m                     \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m                     \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_files_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m                 )\n\u001b[1;32m   1533\u001b[0m             )\n",
      "\u001b[0;31mOSError\u001b[0m: Model name 'IlyaGusev/mbart_ru_sum_gazeta' was not found in tokenizers model name list (facebook/mbart-large-en-ro, facebook/mbart-large-cc25). We assumed 'IlyaGusev/mbart_ru_sum_gazeta' was a path, a model identifier, or url to a directory containing vocabulary files named ['sentencepiece.bpe.model'] but couldn't find such vocabulary files at this path or url."
     ]
    }
   ],
   "source": [
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "\n",
    "article_text = \"...\"\n",
    "model_name = \"IlyaGusev/mbart_ru_sum_gazeta\"\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ecd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
